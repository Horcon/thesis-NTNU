\chapter{Discussion}

%5-10 pages?

This master thesis aims to answer these three research questions that were presented in the introduction:

\begin{itemize}
    \item How do neural networks that are iteratively re-trained using transfer learning on an increasing subset of the dataset, to group the entire dataset they receive into “super-classes” perform? 
    \item How does this training technique perform when used to train neural networks in a tree hierarchy that bases itself on these super-classes?
    \item How does changing the parameters of the training process affect the neural networks and the resulting tree structures?
\end{itemize}{}

Each of the following sections aims to answer how the research conducted in this master thesis provided insight into the potential answer to these questions.

%Pre-text: discussion aims to answer each of the three research questions presented in the introduction
%Repeat the research questions here?

%Section 1: Iterative training in general
\section{Iterative re-training performance}

%In detail discuss how iterative training went, focusing on the aspect of iterative training

%Section 2: Parameter tuning of iterative training
\section{Tree generation process}
%Go into the information found in the parameter search
%Already interesting known result, threshold had significant effect
%0.5 was "good" but few iterations on initial model, 0.6 and 0.7 collapsed into 2 classes, 0.8 and 0.9 kept 3 classes
%Because of time constraints, decision to go with 0.8 was made based on it being a more "balanced" number, faster tree generation, earlier use in development as "assumed good value"

%Section 3: Tree
\section{Combining both items together}

%Discuss the trees generated, the results and how they compare

