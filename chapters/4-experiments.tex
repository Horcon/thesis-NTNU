\chapter{Experiments}

During the work on this thesis, various experiments have been conducted to find the correct parameters to use in subsequent development.
The following section provides details on the experiments done during this thesis to explain how the final results in the thesis have been achieved.


%Section 0: Hardware and software used in the project
%Hardware: Specify CPU/GPU/RAM etc used, available total storage (for flex?)

\section{Tools used in the thesis}
The following section lists the hardware and software used during the experiments in this master thesis.
It also lists tool-specific findings that are not relevant to mention in the other sections.

\subsection{Hardware}
\label{chex:hardware}

A table with the primary hardware used in this thesis can be found in \cref{tab:hardware}.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Part type & Name & Main speed & Total memory size\\ \hline
        GPU & RTX 2080 Ti & 1.65 Ghz & 11 GB GDDR5\\ \hline
        CPU & Intel i7 4790k & 4.00 GHz & 32 GB DDR3\\ \hline
        Motherboard & Z97X-UD5H-BK & & \\ \hline
        SSD & 2x Samsung 860 EVO & 540 MB/s & 2TB \\ \hline
        HDD & 5x 12TB + 1x 10 TB & 160 MB/s & 70 TB \\ \hline
        Off. HDD & 5x 4 TB & 100 MB/s & 20 TB \\ \hline
        
    \end{tabular}
    \caption{Hardware specs}
    \label{tab:hardware}
\end{table}

In addition to the primary hardware, in several steps, processing was accelerated using other available PCs.
While helpful, the presented hardware has proven itself to be much faster than this alternative hardware, as noted in experiment 1 (data processing) and experiment x (iterative train root node). %Paranthesis and experiment numbers and text to be replaced by actual cref when these sections are done

The primary workhorse of the thesis is the GPU, the RTX 2080 Ti\cite{rtx2080}.
When the GPU is under minor load, the default clock is lower than advertised, at 1350 MHz.
The lower clock is due to the adaptive overclock features of the GPU.
Since the neural network processing does not utilize all GPU functions, and smaller models do not use the GPU to their fullest, the operating system lowers the clock speed to reduce heat.
To circumvent this and use the hardware to its fullest extent, one can start multiple Python consoles and train multiple neural networks in parallel.
In the case of the RTX 2080 Ti, up to 5 simultaneous threads have been used stably, with some limitations.
During full utilization of the GPU, clock speeds above 1920 MHz have been recorded, while maintaining temperatures below 60 degrees celsius.
As at that point, resource utilization is at its fullest; network training time is increased for each individual thread. 
However, the total processing time is still reduced as hardware is utilized more overall.

One problem that has arisen during development was the lacking capabilities of other hardware when compared to the GPU.
The RTX 2080 Ti was purchased in late 2019 for the thesis, while the CPU and RAM were purchased in early 2015.
Running multiple Python consoles includes having the cache for each console in RAM.
Python is not a memory-optimized scripting language, and uses much space for each variable in memory\cite{theano:memory}.
While the amount of RAM available on the hardware is above the standard of desktops built even today, maxing the capacity of the motherboard, some experiments have still been limited in running more simultaneous processes due to RAM shortage.
Also, the limitation of the quad-core nature of the CPU has been a limiting factor in the more CPU intensive tasks in the thesis, most notably experiment 1.%cref
Tasks that only run on the CPU like data preparation and even python consoles themselves during neural network processing need a minimum amount of CPU resources to function properly.
Even if the RAM issue was resolved, the limited number of CPUs would block more than 5 python consoles running simultaneously.

In terms of findings on storage and caching, when using Pythons pickle library, storing the dataset on an SSD will improve read times by an estimated 35\% over using HDDs.
According to a speed test done on the hardware, the read-time of an SSD is around 540 MB/s, while the read-time of the tested HDD is around 160 MB/s.
Therefore, the benefits of moving the dataset to a faster medium is entirely dependent on the size of the dataset, and frequency of changes to the dataset.
In the case of this thesis, experiment x (root node) used the SSD as a cache for the dataset.%cref
In contrast, experiment y (trees) used almost all available HDDs for the dataset cache, excluding the external drive.%cref
Given the nature of the last experiment, each experiment variation would need a full copy of the dataset.
Also, these copies of the dataset would be repeatedly created and deleted, adding to the wear of the drive.
Each experiment variation was given a dedicated drive to alleviate the potential performance loss due to drive seeking present on HDDs, with the results also being printed to a dedicated drive.
As mentioned, the external drive was not used as a dedicated drive, due to lower performance compared to the internal drives operating over SATA-3.

It is also important to mention the massive data storage pool of the hardware.
Thanks to the available storage, each step of the dataset generation could be stored for safe-keeping.
Large amounts of free storage also enabled various storage-expensive methods of caching, as mentioned above.
Also, multiple older HDDs have been used as backups and portable storage for the dataset.

\subsection{Software}
%Software: Specify Python + Tensorflow versions (2.0 early on, 2.1 later, 2.2 available in the end but not used)
%Software python: Include info on Librosa, Scikit and other large libraries used and why
The development platform used during this thesis has primarily been Python on a Windows 7 computer.
\cref{tab:software} details the different versions of the software used for development in the thesis.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Name & Version\\ \hline
        Anaconda & 2019.10\\ \hline
        Python & 3.7\\ \hline
        TensorFlow & 2.0, 2.1\\ \hline
        TensorBoard & 2.0, 2.1\\ \hline
        Librosa & 0.7.2\\ \hline
        Scikit-learn & 0.22.2\\ \hline
        Spyder & 3.3.6, 4.1.2\\ \hline
        PowerShell & 5.1\\ \hline
        FFmpeg & 4.2\\ \hline
        
    \end{tabular}
    \caption{Software versions}
    \label{tab:software}
\end{table}

\subsubsection{Python environment}
%Python 3.7
A Python environment requires a package manager to use it to the fullest extent.
Visual Studio that the student has already had installed on the computer also included a full Python environment with Anaconda\footnote{\url{https://www.anaconda.com/}}.
While the included version has proven itself to be unreliable and required multiple re-installations, certain features have proven themselves to be necessary throughout this thesis.
The most crucial feature that prioritized Anaconda over picking pip was the ability to create separate virtual environments with different versions of packages installed simultaneously.
Individual packages often require specific versions of dependencies that may not be available anymore, in which case an outdated version of the package may be installed without notice to the user.

%Librosa for dataset, mention trouble with version in Anaconda
One case of the separate environments being critical for this thesis was the Librosa\footnote{\url{https://librosa.github.io/librosa/index.html}} library.
Librosa is a python package for music and audio analysis, which this thesis used to process the dataset with the various feature extraction methods that Librosa supports.
Deep in the dependency chain for Librosa, a dependency conflict arose that forced Anaconda to install version 0.6.3 of Librosa, while the latest current version is 0.7.2.
As the feature extraction methods are based on scientific algorithms and thus should not change between versions, each new version of a package can include more methods that the user expects to have available.
A separate environment dedicated exclusively for Librosa had to be developed to handle this dependency conflict to resolve the matter in the thesis.


%Tensorflow 2.0, 2.1 later on
The Tensorflow\footnote{\url{https://www.tensorflow.org/}} package was selected for the development of the thesis to create and train neural networks.
As Google develops TensorFlow, one of the leading firms in Artificial Intelligence research that also employs researchers that published the Inception paper\cite{szegedy2014going}, it was picked as the superior choice.
Shortly before the thesis project started the development of neural networks, version 2.0 of Tensorflow was released.
This thesis used version 2.0 at the beginning of the thesis, updating to version 2.1 in the middle of the thesis.
Version 2.2 was released in the final month of the thesis. 
While certain features that were released in this version would be exciting to use during development, it was not used to maintain the stability of the thesis results.

As noted in the hardware section, multiple Python consoles have been used to train multiple neural networks simultaneously.
To achieve this, Tensorflow needs to be configured only to use a limited amount of memory on the GPU.
Under regular operation, Tensorflow will attempt to use all available memory for itself, which will lead to potential system instability and hanging even if only one network is trained at a time.
Because of the recent update from 1.x versions of Tensorflow, a compatibility layer needs to be used in current versions of Tensorflow as the equivalent functionality has not been found in versions 2.0 and higher.
The memory fraction parameter has to be changed to the desired level to adjust the maximum memory used by Tensorflow, code for which is provided in the code listing below.
It is important to note that the memory fraction does not represent the actual memory fraction used by Python, and is a significant percentage higher than the parameter set in the code.
A tool provided by Nvidia called Nvidia SMI had been used to monitor resource utilization during the development of the minimal parameters that could provide a stable and fast environment.

\lstinputlisting[
    caption={Code to reduce memory usage of Tensorflow in a single console, valid for Tensorflow 2.1},
    label=lst:pythonfile,
    language=Python
]{listings/memcode.py}

%Tensorboard for experiment results
The Tensorboard\footnote{\url{https://www.tensorflow.org/tensorboard}} package was used to process the results of the various experiments detailed in the later sections of this chapter.
Tensorboard is a visualization toolkit developed for Tensorflow.
The primary feature used in the toolkit was the ability to record, view, and sort individual neural network tests.
As Tensorboard integrates easily in Tensorflow, this allowed for agile development of the test code, where tests were rapidly coded, run, and analyzed.
However, given the sheer amount of tests run during this thesis, the thesis has stumbled across a critical performance bug.
Tensorboard attempts to record the state of the neural network as it is trained, and recreate a visual graph for the training process.
While this may be a useful feature for other projects, it was not relevant for this thesis, while simultaneously causing some experiments to max out the RAM on the computer.
The features that cause this bug to manifest can be disabled by using Anacondas' capacity to create custom environments, creating an environment exclusively for Tensorboard.
Other packages can also be installed in this environment; however, Tensorflow has to be explicitly excluded from this environment.
Doing so causes Tensorboard to enter a restricted feature mode, which allows for much larger tests to be viewed with minimal lag.

%Scikit-learn for clustering
%Spyder IDE, Anaconda environment
%Pickle for data saving
In addition to the above packages, several other packages have been used extensively in the project.
To create the hierarchical clusters for the first clusters, scikit-learn\footnote{\url{https://scikit-learn.org/stable/index.html}} has been used.
Scikit-learn provides a lot of various functions that are useful in neural network development.
For the IDE, Anaconda comes pre-installed with Spyder\footnote{\url{https://www.spyder-ide.org/}}, which was used for the development of the thesis.
Lastly, the pickle package in Python was used to save results to disk, along with handling cache logic.


\subsubsection{Other}
%Powershell for dataset s1 and s2
%FFMpeg for dataset extraction, processing
In addition to using Python during most of the development, Powershell was used in the early stages of dataset preparation.
Powershell was chosen for these first tasks due to earlier experience in using Powershell to maintain the media library used as the dataset in the thesis.
As Powershell would be satisfactory in the first tasks, it was chosen to be used over Python due to students lacking experience in using Python at the time.
Since the data extraction script was intended to be run only once, writing the scripts quickly and starting to develop Python code was preferable, especially given the expected time to process the dataset.
The Powershell version used in the thesis was 5.1.

The media converter FFmpeg\footnote{\url{https://ffmpeg.org/}} was used to facilitate the processing of the dataset into a standardized form.
FFmpeg supports a wide variety of video, audio, and subtitle formats.
The ability to process any input was critical when various video files in the dataset had an unknown number of codecs used to encode them.
Some of the more recent media may use more modern formats, while older media may use some more obscure format that other tools could fail to process.
As each sample in the final dataset needed to carry only the particular subtitle fragment, the ability to specify file length options with a command line was also necessary for the selection of the conversion tool.
Since losing samples due to shortcomings in the selected tool would be undesirable, FFmpeg fit the requirements the most.
The FFmpeg version used in the thesis was 4.2.



%Section 1: Dataset selection, why mfcc dct 3 and others got picked, here is actual first nn development
\section{Dataset selection}
\label{ex:dataset}

The goal of this experiment was to investigate if neural networks could produce any relevant results for the thesis and determine which of the function combinations would be best suited to proceed with.
\subsection{Experiment setup}
The first experiment revolved around a minimalist neural network to select the appropriate function combination to use in Librosa to generate the dataset for the rest of the thesis.
A table detailing each layer in the network can be seen in \cref{tab:earlymodel}.

Three one-dimensional convolution layers start the network, followed with a standard max pool layer.
An operational flattening layer is added to make the layers fit into the dense layers.
One dense layer is added before the final two layers, to serve as an abstraction of the input data.
Then, a dense layer with the softmax activation function is added to the network.
The softmax layer serves as the classification layer, forcing the network to pick one of the neurons to classify the sample into.
An extra dense layer is added with the sigmoid activation function to represent the dataset
This layer represents the dataset used in this experiment, and each sample maps to their neuron in this layer.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Layer type & Parameters & Activation\\ \hline
        Conv1d & 64 filters, size 3 & ReLU\\ \hline
        Conv1d & 64 filters, size 3 & ReLU\\ \hline
        Conv1d & 64 filters, size 3 & ReLU\\ \hline
        Max pool & Size 2 & ReLU\\ \hline
        Flatten & &\\ \hline
        Dense & 250 & ReLU\\ \hline
        Dense & 10 & Softmax \\ \hline
        Dense & 1000 & Sigmoid \\ \hline
    \end{tabular}
    \caption{Early neural network model}
    \label{tab:earlymodel}
\end{table}

The process of getting the results can be summarized as follows:
\begin{itemize}
    \item Train a fresh network on the entire dataset.
    \item Remove the last layer
    \item Run a prediction on the dataset
    \item Generate a new instance of the network, minus the last layer
    \item Train the network on the dataset using a 70\% train and 30\% validation split
\end{itemize}{}

By creating a new network and training the network based on the results of the first, the goal was to determine if the network was able to find any features to group samples autonomously.
To control for potential overfit errors, in addition to the standard validation loss and accuracy values, the number of classes and the size of the largest class was tracked.
Therefore, the best result would be the function combination that achieved the best performance in all three combinations.

\subsubsection{Dataset}
Using the entire dataset for this experiment was not yet possible, both because it did not exist in the correct form yet, and that the neural network was built to handle only one thousand networks, to begin with.
For this experiment, a selection of around 38 thousand samples were processed in all function combinations.

The function combinations could consist of the following options:

\begin{itemize}
    \item FFT window size: 1024, 2048, 4096
    \item n\_chroma: 12, 24, 48
    \item DCT: 2, 3
    \item Power: 1, 2, 3
    \item Order: 0, 1, 2
    \item Center: True, False
\end{itemize}{}

The functions that were considered and the parameters that were tried with them are presented in \cref{tab:ex1dataset}.
In total, 67 possible combinations were tried.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        Function type & Parameter type\\ \hline
        chroma\_stft & FFT, n\_chroma\\ \hline
        chroma\_cqt & n\_chroma\\ \hline
        chroma\_cens & n\_chroma\\ \hline
        melspectrogram & FFT, Power\\ \hline
        mfcc & DCT\\ \hline
        rms & FFT\\ \hline
        spectral\_centroid & FFT, Center\\ \hline
        spectral\_bandwidth & FFT\\ \hline
        spectral\_contrast & FFT, Center\\ \hline
        spectral\_flatness & FFT, Power\\ \hline
        spectral\_rolloff & FFT\\ \hline
        poly\_features & FFT, Order\\ \hline
        tonnetz & \\ \hline
        zero\_crossing\_rate & \\ \hline
    \end{tabular}
    \caption{Librosa functions that were tested}
    \label{tab:ex1dataset}
\end{table}

\subsubsection{Control group}

A control test was run to verify that the network achieved a useful result.
The control test consisted of training the neural network model on a randomly generated group of classes.
Should the experiment be successful, then the results of the tested networks will easily surpass the control group.
In addition, if the control group scored a high level of accuracy, different problems with the neural network itself could be highlighted.

\subsection{Experiment results}

The results that were used for further work in the thesis are listed in \cref{tab:ex1results}.
While some of the results completely removed some functions entirely, others presented very good results.
As expected, the control group achieved accuracy rates within the proximity of 10\%, which given the ten output classes, means that the network failed to find anything in the control group.

In the experiment, however, the results were much better.
Mel-frequency cepstrum coefficients with the Discrete Cosine Transform set to three provided the best results of all.
While the Constant-Q chromagram has marginally better results in the loss and accuracy, its highest class count is significantly higher.
The higher largest size of the 12 chroma test is caught up by the other two combinations, indicating the potential for improvement.
Spectral contrast has also shown some promising results. 
However, the loss accuracy values are considered to be too perfect on the false parameter test, indicating potential pitfalls in using it as the primary dataset.


\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Function type & Param type & Loss/Acc & Class count and largest size\\ \hline
        Control group & - & - / 9-12\% & - \\ \hline
        MFCC & DCT 2 & 0.6 / 0.86 & 10 - 203\\ \hline
        MFCC & DCT 3 & 0.17 / 0.94 & 8 - 181\\ \hline
        chroma\_cqt & 12 & 0.16 / 0.95 & 8 - 220\\ \hline
        chroma\_cqt & 24 & 0.59 / 0.88 & 10 - 203\\ \hline
        chroma\_cqt & 48 & 0.52 / 0.91 & 10 - 187\\ \hline
        spectral\_contrast & 4096, False & 0.01, 0.99 & 8 - 225\\ \hline
        spectral\_contrast & 4096, True & 0.64, 0.88 & 9 - 204\\ \hline
    \end{tabular}
    \caption{Experiment 1 results}
    \label{tab:ex1results}
\end{table}

Ultimately, these three different function types have been determined to be the best options to use for the rest of the thesis.
Of these, MFCC with DCT 3 was the only one used, as no issues with the dataset have arisen that would require a dataset change.

%Initial dataset of around 38k samples, 5 series picked one from each drive
%Method, initial neural network, 10 neuron softmax + 1000 neuron next layer, generate results, no validation method
%To verify that the results were not overfit, use results of this on nn of same architecture minus 1k layer
%Control version is pure random assignment that produced 9-12% accuracy (purely random, no structure in data for network to detect)
%Important metrics were: highest class size, number of classes (higher is better), vloss and vacc
%Bullet list of all variants tested, mention extra detail about mfcc and why bigger was chosen
%Best in class is mfcc dct 3, followed by c cqt 12, small variants could prove useful of which spec con with fft window set to 4096 was the most relevant
%Because of relationship to best results, dct 2 was also generated for mfcc, all variants of c cqt were generated due to smaller size and better results than 12 for class size
%Primary dataset is based on mfcc dct 3 in this thesis, rest has not been used due to lack of time to consider their use

%Section 2: 1010 neuron idea verification, model based on ex 1
%Following the selection of the dataset, 1010 neuron idea was tested
%Custom layer was developed with the weights adjusted in the last 10 layers
%Pictures from APW report
%Additional theories tested, mention extra options, why they were not chosen (extra workload on processing, absolutely uninteresting results compared to just using custom layer and going forward)
%Important to mention as normal layer with both extra counter-measures appears to come close to custom layer results

%Section 3: Neural network development in spring
%Methodology writeup, tons of combinations
%Section 1 to find good number of conv layers
%Section 2 pre-3-cluster or layer freeze modelling, table with results and subsequent next tests
%Section 3 final model, based on later experience, model parameters did not appear to be all that relevant in the dataset


%Lots of graph pages in this section
%10-20 pages? but lots of pictures

%Section 4: Loss function hyperparams?
%Results justifying why I went with the loss function parameters, the final last result
%Q: Had a "bad" batch of results that maybe weren't as relevant, include but state it used an older version?
%Section 1 on before 3clusters
%Section 2 on before layer freeze attempt
%Section 3 final results, which were "best in class", some had more trouble while others didn't?


%Section 5: Iterative training
%Main section detailing the big results from iterative training, all different ways to arrange data
%Provide as broad of a platform for discussion in the next section
%As results are in next chapters, discuss experiment parameters and how the experiment went?
%More technical part compared to methodology


%Section 6: Tree generation
%Same but for trees
%Uses resulting networks from iter train as root nodes, since it's same shit
%Based on the cluster collapse down to 2 clusters in previous experiment, uses 0.8 as threshold for training
%0.5 as default threshold to be used in next tree, control version -3 could afford smaller numbers
%Time it takes to generate trees, mention how combination 3 finished on FUCKING FRIDAY before report handin?
%As many results as possible for the discussion section

