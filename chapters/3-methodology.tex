\chapter{Methodology}

%Long(est) chapter of all probably, 10-20 pages
%Somewhat historical walkthrough through the thesis project, in order of how I did the tasks
%Pre-section: Explain the methodology-experiment chapter division, agile thesis work
Development of the hypothesis for this master thesis and the research questions has begun during the summer of 2019, after contemplating the results of the project work done in the Image Processing and Analysis course.
Over the following year, various parts of the thesis scope and hypothesis have been adjusted to fit in the allocated time, and based on work done during the thesis.
In the autumn semester of 2019, preliminary work was done to assess the feasibility of the master thesis.
Based on the results of these feasibility studies, the spring semester work was focused on implementing the iterative re-training process and tree hierarchy generation.

The research questions of this thesis are aimed at the final result at the end of the development process. 
However, the reasoning behind the decisions taken during development needs to be documented.
The development of the code used in this thesis was done using agile methods.
After a module was developed, it was repeatedly tested, and based on the results of these tests; the next tests were created.

The following chapter presents the methodology of the development done in this thesis.
For each section that has experiments associated with decisions taken during development, the intent behind the experiment is explained.
As the thesis does not seek to answer the question of how to develop iterative re-training process or tree hierarchy generation, the experiments done during the development of this thesis are documented in chapter 4.



%Section 0: Hardware and software used in the project
%Hardware: Specify CPU/GPU/RAM etc used, available total storage (for flex?)
%Software: Specify Python + Tensorflow versions (2.0 early on, 2.1 later, 2.2 available in the end but not used)
%Software python: Include info on Librosa, Scikit and other large libraries used and why
\section{Tools used in the thesis}
The following section lists the hardware and software used during this master thesis.
It also lists tool-specific findings that are not relevant to mention in the other sections.

\subsection{Hardware}

A table with the primary hardware used in this thesis can be found in \cref{tab:hardware}.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Part type & Name & Main clock & Mem clock & Total memory size\\ \hline
        GPU & RTX 2080 Ti & 1.65 Ghz & 7000 Mhz & 11 GB GDDR5\\ \hline
        CPU & Intel i7 4790k & 4.00 GHz & 1866 Mhz & 32 GB DDR3\\ \hline
        SSD & 2x Samsung 860 EVO & & & 2TB \\ \hline
        HDD & 5x 12TB + 1x 10 TB & & & 70 TB \\ \hline
        Off. HDD & 5x 4 TB & & & 20 TB \\ \hline
        
    \end{tabular}
    \caption{Hardware specs}
    \label{tab:hardware}
\end{table}

In addition to the primary hardware, in several steps, processing was accelerated using other available PCs.
While helpful, the presented hardware has proven itself to be much faster than this alternative hardware, as noted in experiment 1 (data processing) and experiment x (iterative train root node). %Paranthesis and experiment numbers and text to be replaced by actual cref when these sections are done

The primary workhorse of the thesis is the GPU, the RTX 2080 Ti\cite{rtx2080}.
When the GPU is under minor load, the default clock is lower than advertised, at 1350 MHz.
The lower clock is due to the adaptive overclock features of the GPU.
Since the neural network processing does not utilize all GPU functions, and smaller models do not use the GPU to their fullest, the operating system lowers the clock speed to reduce heat.
To circumvent this and use the hardware to its fullest extent, one can start multiple Python consoles and train multiple neural networks in parallel.
In the case of the RTX 2080 Ti, up to 5 simultaneous threads have been used stably, with some limitations.
During full utilization of the GPU, clock speeds above 1920 MHz have been recorded, while maintaining temperatures below 60 degrees celsius.
As at that point, resource utilization is at its fullest; network training time is increased for each individual thread. 
However, the total processing time is still reduced as hardware is utilized more overall.

One problem that has arisen during development was the lacking capabilities of other hardware when compared to the GPU.
The RTX 2080 Ti was purchased in late 2019 for the thesis, while the CPU and RAM were purchased in early 2015.
Running multiple Python consoles includes having the cache for each console in RAM.
Python is not a memory-optimized scripting language, and uses much space for each variable in memory.%cite
While the amount of RAM available on the hardware is above the standard of desktops built even today, maxing the capacity of the motherboard, some experiments have still been limited in running more simultaneous processes due to RAM shortage.
Also, the limitation of the quad-core nature of the CPU has been a limiting factor in the more CPU intensive tasks in the thesis, most notably experiment 1.%cref
Tasks that only run on the CPU like data preparation and even python consoles themselves during neural network processing need a minimum amount of CPU resources to function properly.
Even if the RAM issue was resolved, the limited number of CPUs would block more than 5 python consoles running simultaneously.

In terms of findings on storage and caching, when using Pythons pickle library, storing the dataset on an SSD will improve read times by an estimated 35\% over using HDDs.
According to a speed test done on the hardware, the read-time of an SSD is around 540 MB/s, while the read-time of the tested HDD is around 160 MB/s.
Therefore, the benefits of moving the dataset to a faster medium is entirely dependent on the size of the dataset, and frequency of changes to the dataset.
In the case of this thesis, experiment x (root node) used the SSD as a cache for the dataset.%cref
In contrast, experiment y (trees) used almost all available HDDs for the dataset cache, excluding the external drive.%cref
Given the nature of the last experiment, each experiment variation would need a full copy of the dataset.
Also, these copies of the dataset would be repeatedly created and deleted, adding to the wear of the drive.
Each experiment variation was given a dedicated drive to alleviate the potential performance loss due to drive seeking present on HDDs, with the results also being printed to a dedicated drive.
As mentioned, the external drive was not used as a dedicated drive, due to lower performance compared to the internal drives operating over SATA-3.

It is also important to mention the massive data storage pool of the hardware.
Thanks to the available storage, each step of the dataset generation could be stored for safe-keeping.
Large amounts of free storage also enabled various storage-expensive methods of caching, as mentioned above.
Also, multiple older HDDs have been used as backups and portable storage for the dataset.

\subsection{Software}



%Section 1: Dataset (5+ pages)
%Detailed walkthrough of what I used as my dataset, processing it
%Use the "Stage 1, 2, 3, 4" terminology used in the dataset preparation
%Include results from Specialization in mobile - no, experiment 1
%Refer to experiment 1 in Stage 3, experiment on why MFCC DCT 3 and the others were selected, with md3 being the primary


%Section 2: Neural network used
%Two "main" sections, first section from autumn semester, second from spring semester
%First section define the early "model" built with the help of marine paper
%First section refers to experiment 2, which details on the results for the custom layer
%Second section refers to experiment 2 producing good results, final layer being used
%Selection of the network model done by repeatedly attempting multiple combinations of parameters
%Display a list of parameters tested

%Detail the methodology used in creating the "base" neural network used in this thesis

%Section 3: Loss function
%In detail provide information on how I created my custom loss function filters, tuned the parameters
%Here is NOT the reason I went down from 10 classes to 3
%Explain the logic behind the filter types, show graphs that can illustrate some of this
%Generate graph with gnuplot like in example, more natural?
%Explain the logic behind the filter values, why these in particular

%Q: Provide this info 10->3 in methods or results? IMO methods but can see how it would be relevant in results

%Section 4: Iterative training
%First of the two sort of relevant to the results sections
%Specify the parameters used in determining the correct parameters in developing iterative training
%Specify what loss functions and parameters would used in iterative training depending on what results

%Section 5: Tree generation
%Tree generation, same as iterative training in terms of specifying what I did, results of this goes into results/discussion
%Specify what iterative training parameters will be used depending on the criteria in the results
